{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(f\"\\nüîç PyTorch: {torch.__version__}\")\n",
    "# print(f\"üîç GPU: {torch.cuda.is_available()}\")\n",
    "# print(f\"üîç Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# # Nova forma de verificar ROCm a partir do PyTorch 2.0+\n",
    "# print(f\"üîç Backend GPU: {'ROCm' if torch.version.hip else 'CUDA'}\")\n",
    "# print(f\"üîç Vers√£o HIP: {torch.version.hip}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {},
   "source": [
    "# Carregando Pacotes e Tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91517be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0def06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",  # Modelo em ingl√™s\n",
    "    tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098531e",
   "metadata": {},
   "source": [
    "# Tratamento do Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "    \"\"\"Carrega o texto de um PDF, removendo espa√ßos extras e p√°ginas vazias.\"\"\"\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages \n",
    "            if page.extract_text()  # Ignora p√°ginas sem texto\n",
    "        )\n",
    "        texto = \" \".join(texto.split())  # Normaliza espa√ßos\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "    \"\"\"Divide o texto em chunks tokenizados, respeitando o limite do modelo.\"\"\"\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    chunk_size = 400\n",
    "    overlap = 100\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a19e2",
   "metadata": {},
   "source": [
    "# Defini√ß√£o de Processamento das Perguntas e das Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb22770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "    \"\"\"\n",
    "    Processa a pergunta sobre os chunks do documento e retorna a melhor resposta.\n",
    "    \n",
    "    Args:\n",
    "        pergunta (str): Pergunta do usu√°rio.\n",
    "        chunks (list): Lista de chunks do documento (j√° tokenizados).\n",
    "        qa_pipeline: Pipeline de Q&A carregado (Bloco 2).\n",
    "        tokenizer: Tokenizador do modelo.\n",
    "        min_score (float): Score m√≠nimo para considerar uma resposta v√°lida.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"answer\": str, \"score\": float, \"context\": str} ou None se nenhuma resposta for encontrada.\n",
    "    \"\"\"\n",
    "    chunks = dividir_em_chunks_tokenizados(documento)\n",
    "    \n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > 60:  # Limite arbitr√°rio (ajuste conforme necess√°rio)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precis√£o.\")\n",
    "    \n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500])  # Fallback: 500 primeiros chars\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.1]\n",
    "    if not respostas_validas:\n",
    "        print(\"N√£o h√° resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "    \n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {},
   "source": [
    "# Pergunta e Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef83c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"arquivo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82668a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pergunta sobre o documento\n",
    "pergunta = \"What is the main topic of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21aff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resposta gerada\n",
    "resposta = processar_pergunta(pergunta, documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
