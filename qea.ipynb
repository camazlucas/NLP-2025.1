{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(f\"\\nüîç PyTorch: {torch.__version__}\")\n",
    "# print(f\"üîç GPU: {torch.cuda.is_available()}\")\n",
    "# print(f\"üîç Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# # Nova forma de verificar ROCm a partir do PyTorch 2.0+\n",
    "# print(f\"üîç Backend GPU: {'ROCm' if torch.version.hip else 'CUDA'}\")\n",
    "# print(f\"üîç Vers√£o HIP: {torch.version.hip}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {},
   "source": [
    "# Carregando Pacotes e Tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91517be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0def06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790beb4",
   "metadata": {},
   "source": [
    "# Vari√°veis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "930bedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tamanho da Janela de Contexto\n",
    "window_size=400\n",
    "\n",
    "#Overlap da Janela de Contexto\n",
    "stride=100\n",
    "\n",
    "#Tamanho da Chunk\n",
    "chunk_size = 400\n",
    "    \n",
    "#Overlap da Chunk\n",
    "overlap = 100\n",
    "\n",
    "#Tamanho M√°ximo da Pergunta\n",
    "tamanho_pergunta = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7515dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_tokenizador = \"pierreguillou/bert-large-cased-squad-v1.1-portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098531e",
   "metadata": {},
   "source": [
    "# Tratamento do Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23947fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_tokenizador)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model= modelo_tokenizador,  # Modelo em Portugues\n",
    "    tokenizer=modelo_tokenizador,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f649732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages \n",
    "            if page.extract_text()  # Ignora p√°ginas sem texto\n",
    "        )\n",
    "        texto = \" \".join(texto.split())  # Normaliza espa√ßos\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d4574",
   "metadata": {},
   "source": [
    "## Divis√£o em Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b51d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c80ff4",
   "metadata": {},
   "source": [
    "## Janela Deslizante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c499a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenizados(texto):\n",
    "    \"\"\"\n",
    "    Divide o texto em janelas deslizantes de tokens com overlap.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto completo.\n",
    "        tokenizer: Tokenizador do modelo.\n",
    "        window_size (int): Tamanho da janela em tokens.\n",
    "        stride (int): Passo (overlap = window_size - stride).\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Lista de janelas de texto.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "    \n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela)\n",
    "        janelas.append(texto_janela)\n",
    "        \n",
    "        # Para se chegarmos ao final do texto\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe809",
   "metadata": {},
   "source": [
    "# Sele√ß√£o da Divisao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e24679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisao em Janela Deslizante\n",
    "\n",
    "divisao_do_texto = sliding_window_tokenizados(documento)\n",
    "\n",
    "#Divisao em Chunks\n",
    "\n",
    "#divisao_do_texto = dividir_em_chunks_tokenizados(documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a19e2",
   "metadata": {},
   "source": [
    "# Defini√ß√£o de Processamento das Perguntas e das Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1eb22770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "    \n",
    "    chunks = divisao_do_texto\n",
    "    \n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > tamanho_pergunta:  # Limite arbitr√°rio (ajuste conforme necess√°rio)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precis√£o.\")\n",
    "    \n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500])  # Fallback: 500 primeiros chars\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.2]\n",
    "    if not respostas_validas:\n",
    "        print(\"N√£o h√° resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "    \n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {},
   "source": [
    "# Pergunta e Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fef83c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"cabelos cacheados.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82668a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pergunta sobre o documento\n",
    "pergunta = \"Quais s√£o as classifica√ß√µes dos tipos de Cachos?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21aff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9076535701751709, 'start': 1599, 'end': 1615, 'answer': 'tipos e subtipos'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Resposta gerada\n",
    "resposta = processar_pergunta(pergunta, documento)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
