{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {
    "id": "e5b99937"
   },
   "source": [
    "# Carregando Pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "HmOrX0lBHfEp",
   "metadata": {
    "collapsed": true,
    "id": "HmOrX0lBHfEp"
   },
   "outputs": [],
   "source": [
    "#pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91517be",
   "metadata": {
    "id": "f91517be"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0def06",
   "metadata": {
    "id": "8f0def06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488390c",
   "metadata": {},
   "source": [
    "# Fun√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f649732e",
   "metadata": {
    "id": "f649732e"
   },
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages\n",
    "            if page.extract_text()\n",
    "        )\n",
    "        texto = \" \".join(texto.split())\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b51d29f",
   "metadata": {
    "id": "3b51d29f"
   },
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c499a487",
   "metadata": {
    "id": "c499a487"
   },
   "outputs": [],
   "source": [
    "def sliding_window_tokenizados(texto):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela)\n",
    "        janelas.append(texto_janela)\n",
    "\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb22770",
   "metadata": {
    "id": "1eb22770"
   },
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "\n",
    "    chunks = divisao_do_texto\n",
    "\n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > tamanho_pergunta:  # Limite arbitr√°rio (ajuste conforme necess√°rio)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precis√£o.\")\n",
    "\n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500])  # Fallback: 500 primeiros chars\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.2]\n",
    "    if not respostas_validas:\n",
    "        print(\"N√£o h√° resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "\n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790beb4",
   "metadata": {
    "id": "3790beb4"
   },
   "source": [
    "# Vari√°veis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7515dfe3",
   "metadata": {
    "id": "7515dfe3"
   },
   "outputs": [],
   "source": [
    "modelo_tokenizador = \"pierreguillou/bert-large-cased-squad-v1.1-portuguese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930bedab",
   "metadata": {
    "id": "930bedab"
   },
   "outputs": [],
   "source": [
    "#Tamanho da Janela de Contexto\n",
    "window_size=400\n",
    "\n",
    "#Overlap da Janela de Contexto\n",
    "stride=100\n",
    "\n",
    "#Tamanho da Chunk\n",
    "chunk_size = 400\n",
    "\n",
    "#Overlap da Chunk\n",
    "overlap = 100\n",
    "\n",
    "#Tamanho M√°ximo da Pergunta\n",
    "tamanho_pergunta = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23947fa1",
   "metadata": {
    "id": "23947fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_tokenizador)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model= modelo_tokenizador,  # Modelo em Portugues\n",
    "    tokenizer=modelo_tokenizador,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {
    "id": "4000ccf7"
   },
   "source": [
    "# Pergunta e Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef83c3e",
   "metadata": {
    "id": "fef83c3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"Cabelo Cacheado Maior.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "h89gr53Mi5cb",
   "metadata": {
    "id": "h89gr53Mi5cb"
   },
   "outputs": [],
   "source": [
    "#Divisao em Janela Deslizante\n",
    "\n",
    "#divisao_do_texto = sliding_window_tokenizados(documento)\n",
    "\n",
    "#Divisao em Chunks\n",
    "\n",
    "divisao_do_texto = dividir_em_chunks_tokenizados(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82668a8d",
   "metadata": {
    "id": "82668a8d"
   },
   "outputs": [],
   "source": [
    "#Pergunta sobre o documento\n",
    "pergunta = \"Como √© a estrutura do cacho tipo 4?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21aff3c",
   "metadata": {
    "id": "e21aff3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8076817989349365, 'start': 840, 'end': 850, 'answer': 'espiralada'}\n"
     ]
    }
   ],
   "source": [
    "#Resposta gerada\n",
    "resposta = processar_pergunta(pergunta, documento)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
