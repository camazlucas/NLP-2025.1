{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(f\"\\nüîç PyTorch: {torch.__version__}\")\n",
    "# print(f\"üîç GPU: {torch.cuda.is_available()}\")\n",
    "# print(f\"üîç Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# # Nova forma de verificar ROCm a partir do PyTorch 2.0+\n",
    "# print(f\"üîç Backend GPU: {'ROCm' if torch.version.hip else 'CUDA'}\")\n",
    "# print(f\"üîç Vers√£o HIP: {torch.version.hip}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {},
   "source": [
    "# Carregando Pacotes e Tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91517be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0def06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Carregar modelo BERTimbau pr√©-treinado para Q&A em Ingles\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# qa_pipeline = pipeline(\n",
    "#     \"question-answering\",\n",
    "#     model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",  # Modelo em ingl√™s\n",
    "#     tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23947fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497a4cee416a40b2bf9aa54c6e82ff3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e6c6db85e048adb588b8a6dfc79390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/918 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ff942116dc4f85bf7911536edd8bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b370803b9243be9224d35e4f483d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769029fe7ee943db8213e3e5a5e72bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f5efa1789b4029a56d2b92ec3b0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/bert-large-cased-squad-v1.1-portuguese\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"pierreguillou/bert-large-cased-squad-v1.1-portuguese\",  # Modelo em Portugues\n",
    "    tokenizer=\"pierreguillou/bert-large-cased-squad-v1.1-portuguese\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098531e",
   "metadata": {},
   "source": [
    "# Tratamento do Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f649732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages \n",
    "            if page.extract_text()  # Ignora p√°ginas sem texto\n",
    "        )\n",
    "        texto = \" \".join(texto.split())  # Normaliza espa√ßos\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b51d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    chunk_size = 400\n",
    "    overlap = 100\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a19e2",
   "metadata": {},
   "source": [
    "# Defini√ß√£o de Processamento das Perguntas e das Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eb22770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "    chunks = dividir_em_chunks_tokenizados(documento)\n",
    "    \n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > 60:  # Limite arbitr√°rio (ajuste conforme necess√°rio)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precis√£o.\")\n",
    "    \n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500])  # Fallback: 500 primeiros chars\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.1]\n",
    "    if not respostas_validas:\n",
    "        print(\"N√£o h√° resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "    \n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {},
   "source": [
    "# Pergunta e Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fef83c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"Cabelo Cacheado Maior.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82668a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pergunta sobre o documento\n",
    "pergunta = \"Qual o t√≠tulo do texto?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e21aff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√£o h√° resposta sobre isto nesse documento.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Resposta gerada\n",
    "resposta = processar_pergunta(pergunta, documento)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
