{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {
    "id": "e5b99937"
   },
   "source": [
    "# Carregando Pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "HmOrX0lBHfEp",
   "metadata": {
    "collapsed": true,
    "id": "HmOrX0lBHfEp"
   },
   "outputs": [],
   "source": [
    "#pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91517be",
   "metadata": {
    "id": "f91517be"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import modelo_eqa as eqa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0def06",
   "metadata": {
    "id": "8f0def06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488390c",
   "metadata": {},
   "source": [
    "# Fun√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f649732e",
   "metadata": {
    "id": "f649732e"
   },
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages\n",
    "            if page.extract_text()\n",
    "        )\n",
    "        texto = \" \".join(texto.split())\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b51d29f",
   "metadata": {
    "id": "3b51d29f"
   },
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c499a487",
   "metadata": {
    "id": "c499a487"
   },
   "outputs": [],
   "source": [
    "def sliding_window_tokenizados(texto):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela)\n",
    "        janelas.append(texto_janela)\n",
    "\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb22770",
   "metadata": {
    "id": "1eb22770"
   },
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "\n",
    "    chunks = divisao_do_texto\n",
    "\n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > tamanho_pergunta:  # Limite arbitr√°rio (ajuste conforme necess√°rio)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precis√£o.\")\n",
    "\n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500]),  # Fallback: 500 primeiros chars\n",
    "                'chunk_completo': chunk\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.2]\n",
    "    if not respostas_validas:\n",
    "        print(\"N√£o h√° resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431a2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extrair_trecho_com_palavras(texto, resposta, palavras_ao_redor=30):\n",
    "    # Encontra a posi√ß√£o da resposta no texto\n",
    "    start_idx = texto.find(resposta)\n",
    "    if start_idx == -1:\n",
    "        return texto.split()[:palavras_ao_redor]  # Fallback: primeiras 30 palavras\n",
    "    \n",
    "    # Extrai palavras antes e depois da resposta\n",
    "    palavras = texto.split()\n",
    "    palavras_resposta = resposta.split()\n",
    "    \n",
    "    # Encontra o √≠ndice aproximado da resposta no texto dividido por palavras\n",
    "    for i in range(len(palavras) - len(palavras_resposta) + 1):\n",
    "        if palavras[i:i+len(palavras_resposta)] == palavras_resposta:\n",
    "            start_word_idx = i\n",
    "            break\n",
    "    else:\n",
    "        return \" \".join(palavras[:palavras_ao_redor])  # Fallback\n",
    "    \n",
    "    # Calcula os √≠ndices do trecho desejado\n",
    "    inicio = max(0, start_word_idx - palavras_ao_redor)\n",
    "    fim = min(len(palavras), start_word_idx + len(palavras_resposta) + palavras_ao_redor)\n",
    "    \n",
    "    return \" \".join(palavras[inicio:fim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2102cd",
   "metadata": {},
   "source": [
    "# Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7515dfe3",
   "metadata": {
    "id": "7515dfe3"
   },
   "outputs": [],
   "source": [
    "#Portugues\n",
    "# modelo_tokenizador = \"mrm8488/distilbert-multi-finedtuned-squad-pt\"\n",
    "modelo_tokenizador = \"pierreguillou/bert-large-cased-squad-v1.1-portuguese\"\n",
    "# modelo_tokenizador = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    "\n",
    "#Multilingual\n",
    "# modelo_tokenizador = \"Khanh/bert-base-multilingual-cased-finetuned-squad\"\n",
    "\n",
    "#Ingles\n",
    "# modelo_tokenizador = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/roberta-large-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"distilbert/distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "# modelo_tokenizador = \"sjrhuschlee/flan-t5-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/electra-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/bert-base-uncased-squad2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790beb4",
   "metadata": {
    "id": "3790beb4"
   },
   "source": [
    "# Vari√°veis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "930bedab",
   "metadata": {
    "id": "930bedab"
   },
   "outputs": [],
   "source": [
    "#Tamanho da Janela de Contexto\n",
    "window_size=400\n",
    "\n",
    "#Overlap da Janela de Contexto\n",
    "stride=100\n",
    "\n",
    "#Tamanho da Chunk\n",
    "chunk_size = 400\n",
    "\n",
    "#Overlap da Chunk\n",
    "overlap = 100\n",
    "\n",
    "#Tamanho M√°ximo da Pergunta\n",
    "tamanho_pergunta = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23947fa1",
   "metadata": {
    "id": "23947fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_tokenizador)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model= modelo_tokenizador,  # Modelo em Portugues\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {
    "id": "4000ccf7"
   },
   "source": [
    "# Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fef83c3e",
   "metadata": {
    "id": "fef83c3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"Cabelo Cacheado Maior.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "h89gr53Mi5cb",
   "metadata": {
    "id": "h89gr53Mi5cb"
   },
   "outputs": [],
   "source": [
    "#Divisao em Janela Deslizante\n",
    "\n",
    "# divisao_do_texto = sliding_window_tokenizados(documento)\n",
    "\n",
    "#Divisao em Chunks\n",
    "\n",
    "divisao_do_texto = dividir_em_chunks_tokenizados(documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5c0dc",
   "metadata": {},
   "source": [
    "# Divis√£o em Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a01bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_com_palavras(texto):\n",
    "    tokens = tokenizer.tokenize(texto)  # Tokeniza o texto\n",
    "    chunks_info = []  # Armazenar√° os chunks e suas palavras\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_texto = tokenizer.convert_tokens_to_string(chunk_tokens)  # Converte para string\n",
    "        \n",
    "        # Extrai as 3 primeiras e √∫ltimas PALAVRAS (n√£o tokens)\n",
    "        palavras = chunk_texto.split()  # Divide o chunk em palavras (por espa√ßos)\n",
    "        \n",
    "        primeiras = ' '.join(palavras[:3]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        ultimas = ' '.join(palavras[-3:]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        \n",
    "        chunks_info.append({\n",
    "            'chunk_completo': chunk_texto,\n",
    "            'inicio': primeiras,\n",
    "            'fim': ultimas\n",
    "        })\n",
    "    \n",
    "    return chunks_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8041ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "3 primeiras palavras: A Defini√ß√£o e\n",
      "3 √∫ltimas palavras: ressaltar que essa\n",
      "Chunk 1\n",
      "3 primeiras palavras: ##√≠culo, mais apertada\n",
      "3 √∫ltimas palavras: em comunidades negras,\n",
      "Chunk 2\n",
      "3 primeiras palavras: O Tipo 4,\n",
      "3 √∫ltimas palavras: atrito que causa\n",
      "Chunk 3\n",
      "3 primeiras palavras: exemplo, √© amplamente\n",
      "3 √∫ltimas palavras: biol√≥gica, cultural ou\n",
      "Chunk 4\n",
      "3 primeiras palavras: o \" devacut\n",
      "3 √∫ltimas palavras: que eles representam.\n"
     ]
    }
   ],
   "source": [
    "# Divide o texto e pega palavras iniciais/finais\n",
    "chunks = dividir_em_chunks_com_palavras(documento)\n",
    "\n",
    "# Exemplo de sa√≠da para o primeiro chunk\n",
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    print(\"3 primeiras palavras:\", chunks[i]['inicio'])\n",
    "    print(\"3 √∫ltimas palavras:\", chunks[i]['fim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe4818",
   "metadata": {},
   "source": [
    "# Divis√£o em Janelas Deslizantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2cefa953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_com_palavras(texto):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela_tokens = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela_tokens)  # Converte para string\n",
    "        \n",
    "        # Extrai as 3 primeiras e √∫ltimas PALAVRAS (n√£o tokens)\n",
    "        palavras = texto_janela.split()  # Divide o texto da janela em palavras\n",
    "        \n",
    "        primeiras = ' '.join(palavras[:3]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        ultimas = ' '.join(palavras[-3:]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        \n",
    "        janelas.append({\n",
    "            'janela_completa': texto_janela,\n",
    "            'inicio_3_palavras': primeiras,\n",
    "            'fim_3_palavras': ultimas\n",
    "        })\n",
    "\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c81af528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "3 primeiras palavras: A Defini√ß√£o e\n",
      "3 √∫ltimas palavras: ressaltar que essa\n",
      "Chunk 1\n",
      "3 primeiras palavras: variados quanto as\n",
      "3 √∫ltimas palavras: ), que j√°\n",
      "Chunk 2\n",
      "3 primeiras palavras: crescem a partir\n",
      "3 √∫ltimas palavras: t√©cnicas de finaliza√ß√£o.\n",
      "Chunk 3\n",
      "3 primeiras palavras: ##√≠culo, mais apertada\n",
      "3 √∫ltimas palavras: em comunidades negras,\n",
      "Chunk 4\n",
      "3 primeiras palavras: categoriza√ß√£o n√£o √©\n",
      "3 √∫ltimas palavras: os √≥leos naturais\n",
      "Chunk 5\n",
      "3 primeiras palavras: come√ßam a formar\n",
      "3 √∫ltimas palavras: Oil ), por\n",
      "Chunk 6\n",
      "3 primeiras palavras: O Tipo 4,\n",
      "3 √∫ltimas palavras: atrito que causa\n",
      "Chunk 7\n",
      "3 primeiras palavras: onde o cabelo\n",
      "3 √∫ltimas palavras: extremos, permanentemente. Cultural\n",
      "Chunk 8\n",
      "3 primeiras palavras: produzidos pelo couro\n",
      "3 √∫ltimas palavras: cachos ( como\n",
      "Chunk 9\n",
      "3 primeiras palavras: exemplo, √© amplamente\n",
      "3 √∫ltimas palavras: biol√≥gica, cultural ou\n",
      "Chunk 10\n",
      "3 primeiras palavras: o frizz. A\n",
      "3 √∫ltimas palavras: que eles representam.\n"
     ]
    }
   ],
   "source": [
    "# Divide o texto e pega palavras iniciais/finais\n",
    "janelas = sliding_window_com_palavras(documento)\n",
    "\n",
    "# Exemplo de sa√≠da para o primeiro chunk\n",
    "for i in range(len(janelas)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    print(\"3 primeiras palavras:\", janelas[i]['inicio_3_palavras'])\n",
    "    print(\"3 √∫ltimas palavras:\", janelas[i]['fim_3_palavras'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rocm_py312",
   "language": "python",
   "name": "venv_rocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
