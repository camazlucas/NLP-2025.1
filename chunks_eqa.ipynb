{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b99937",
   "metadata": {
    "id": "e5b99937"
   },
   "source": [
    "# Carregando Pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "HmOrX0lBHfEp",
   "metadata": {
    "collapsed": true,
    "id": "HmOrX0lBHfEp"
   },
   "outputs": [],
   "source": [
    "#pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91517be",
   "metadata": {
    "id": "f91517be"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import modelo_eqa as eqa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0def06",
   "metadata": {
    "id": "8f0def06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"🔧 Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488390c",
   "metadata": {},
   "source": [
    "# Funções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f649732e",
   "metadata": {
    "id": "f649732e"
   },
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages\n",
    "            if page.extract_text()\n",
    "        )\n",
    "        texto = \" \".join(texto.split())\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b51d29f",
   "metadata": {
    "id": "3b51d29f"
   },
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c499a487",
   "metadata": {
    "id": "c499a487"
   },
   "outputs": [],
   "source": [
    "def sliding_window_tokenizados(texto):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela)\n",
    "        janelas.append(texto_janela)\n",
    "\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb22770",
   "metadata": {
    "id": "1eb22770"
   },
   "outputs": [],
   "source": [
    "def processar_pergunta(pergunta, documento):\n",
    "\n",
    "    chunks = divisao_do_texto\n",
    "\n",
    "    # 1. Tokeniza a pergunta para verificar tamanho\n",
    "    tokens_pergunta = tokenizer.tokenize(pergunta)\n",
    "    if len(tokens_pergunta) > tamanho_pergunta:  # Limite arbitrário (ajuste conforme necessário)\n",
    "        print(\"Pergunta muito longa! Simplifique para melhor precisão.\")\n",
    "\n",
    "    # 2. Executa Q&A em cada chunk\n",
    "    respostas = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resposta = qa_pipeline(question=pergunta, context=chunk)\n",
    "            respostas.append(resposta)\n",
    "\n",
    "            resposta_completa = {\n",
    "                'answer': resposta.get('answer', ''),\n",
    "                'score': resposta.get('score', 0),\n",
    "                'context': resposta.get('context', chunk[:500]),  # Fallback: 500 primeiros chars\n",
    "                'chunk_completo': chunk\n",
    "            }\n",
    "            respostas.append(resposta_completa)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no chunk: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 3. Filtra respostas com score baixo e seleciona a melhor\n",
    "    respostas_validas = [r for r in respostas if r['score'] >= 0.2]\n",
    "    if not respostas_validas:\n",
    "        print(\"Não há resposta sobre isto nesse documento.\")\n",
    "        return None\n",
    "    return max(respostas_validas, key=lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431a2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extrair_trecho_com_palavras(texto, resposta, palavras_ao_redor=30):\n",
    "    # Encontra a posição da resposta no texto\n",
    "    start_idx = texto.find(resposta)\n",
    "    if start_idx == -1:\n",
    "        return texto.split()[:palavras_ao_redor]  # Fallback: primeiras 30 palavras\n",
    "    \n",
    "    # Extrai palavras antes e depois da resposta\n",
    "    palavras = texto.split()\n",
    "    palavras_resposta = resposta.split()\n",
    "    \n",
    "    # Encontra o índice aproximado da resposta no texto dividido por palavras\n",
    "    for i in range(len(palavras) - len(palavras_resposta) + 1):\n",
    "        if palavras[i:i+len(palavras_resposta)] == palavras_resposta:\n",
    "            start_word_idx = i\n",
    "            break\n",
    "    else:\n",
    "        return \" \".join(palavras[:palavras_ao_redor])  # Fallback\n",
    "    \n",
    "    # Calcula os índices do trecho desejado\n",
    "    inicio = max(0, start_word_idx - palavras_ao_redor)\n",
    "    fim = min(len(palavras), start_word_idx + len(palavras_resposta) + palavras_ao_redor)\n",
    "    \n",
    "    return \" \".join(palavras[inicio:fim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2102cd",
   "metadata": {},
   "source": [
    "# Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7515dfe3",
   "metadata": {
    "id": "7515dfe3"
   },
   "outputs": [],
   "source": [
    "#Portugues\n",
    "# modelo_tokenizador = \"mrm8488/distilbert-multi-finedtuned-squad-pt\"\n",
    "# modelo_tokenizador = \"pierreguillou/bert-large-cased-squad-v1.1-portuguese\"\n",
    "# modelo_tokenizador = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    "\n",
    "#Multilingual\n",
    "# modelo_tokenizador = \"Khanh/bert-base-multilingual-cased-finetuned-squad\"\n",
    "\n",
    "#Ingles\n",
    "# modelo_tokenizador = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/roberta-large-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"distilbert/distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "# modelo_tokenizador = \"sjrhuschlee/flan-t5-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/electra-base-squad2\"\n",
    "\n",
    "# modelo_tokenizador = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n",
    "\n",
    "modelo_tokenizador = \"deepset/bert-base-uncased-squad2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790beb4",
   "metadata": {
    "id": "3790beb4"
   },
   "source": [
    "# Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930bedab",
   "metadata": {
    "id": "930bedab"
   },
   "outputs": [],
   "source": [
    "#Tamanho da Janela de Contexto\n",
    "window_size=400\n",
    "\n",
    "#Overlap da Janela de Contexto\n",
    "stride=100\n",
    "\n",
    "#Tamanho da Chunk\n",
    "chunk_size = 400\n",
    "\n",
    "#Overlap da Chunk\n",
    "overlap = 100\n",
    "\n",
    "#Tamanho Máximo da Pergunta\n",
    "tamanho_pergunta = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23947fa1",
   "metadata": {
    "id": "23947fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pré-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_tokenizador)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model= modelo_tokenizador,  # Modelo em Portugues\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000ccf7",
   "metadata": {
    "id": "4000ccf7"
   },
   "source": [
    "# Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef83c3e",
   "metadata": {
    "id": "fef83c3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "#Carrega o documento em PDF\n",
    "\n",
    "documento = carregar_documento(\"Curly Hair.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "h89gr53Mi5cb",
   "metadata": {
    "id": "h89gr53Mi5cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#Divisao em Janela Deslizante\n",
    "\n",
    "# divisao_do_texto = sliding_window_tokenizados(documento)\n",
    "\n",
    "#Divisao em Chunks\n",
    "\n",
    "divisao_do_texto = dividir_em_chunks_tokenizados(documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5c0dc",
   "metadata": {},
   "source": [
    "# Pergunta e Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82668a8d",
   "metadata": {
    "id": "82668a8d"
   },
   "outputs": [],
   "source": [
    "# #Pergunta sobre o documento\n",
    "# pergunta = \"How are curls classified?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e21aff3c",
   "metadata": {
    "id": "e21aff3c"
   },
   "outputs": [],
   "source": [
    "# #Resposta gerada\n",
    "# resposta = processar_pergunta(pergunta, documento)\n",
    "# print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a01bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_com_palavras(texto):\n",
    "    tokens = tokenizer.tokenize(texto)  # Tokeniza o texto\n",
    "    chunks_info = []  # Armazenará os chunks e suas palavras\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_texto = tokenizer.convert_tokens_to_string(chunk_tokens)  # Converte para string\n",
    "        \n",
    "        # Extrai as 3 primeiras e últimas PALAVRAS (não tokens)\n",
    "        palavras = chunk_texto.split()  # Divide o chunk em palavras (por espaços)\n",
    "        \n",
    "        primeiras = ' '.join(palavras[:3]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        ultimas = ' '.join(palavras[-3:]) if len(palavras) >= 3 else ' '.join(palavras)\n",
    "        \n",
    "        chunks_info.append({\n",
    "            'chunk_completo': chunk_texto,\n",
    "            'inicio': primeiras,\n",
    "            'fim': ultimas\n",
    "        })\n",
    "    \n",
    "    return chunks_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk completo: the definition and diversity of curly hair : a closer look at curvature, care, and cultural significance curly hair represents one of the most fascinating expressions of human diversity, not just as a physical trait, but as an element loaded with identity, history, and culture. its unique structure is the result of a complex combination of genetic, environmental, and even emotional factors that shape curl patterns as varied as the personalities that bear them. to truly understand what defines curly hair, one must delve into the science behind the strands, the classifications that organize them, and the care that keeps them healthy, as well as recognizing the social role they play in different communities around the world. curls begin to form in the hair follicle, the microscopic structure in the skin responsible for producing each strand of hair. unlike straight hair, which grows from perfectly round and symmetrical follicles, curly hair arises from follicles with asymmetrical shapes, often elliptical or even kidney - shaped. this irregularity causes keratin, a protein that makes up the hair fiber, to be distributed unevenly along the strand, creating tension zones that force the hair to curl in on itself. the flatter or more oval the follicle, the tighter the resulting curl, which explains why some curls are wide and loose, while others curl into almost microscopic spirals. the classification of curls into types and subtypes was popularized by the system created by stylist beverly johnson, who categorized hair into four main groups ( type 1 to 4 ), with subdivisions that reflect degrees of curvature. however, it is important to emphasize that this categorization is not rigid — many individuals have more than one type of curl on their head, a mix that defies standards and requires personalized care. type 2 hair, for example, is considered wavy, moving between straight and curly. they usually have a soft “ s ” shape, but can range from barely noticeable waves ( 2a ) to more defined and voluminous\n",
      "3 primeiras palavras: the definition and\n",
      "3 últimas palavras: defined and voluminous\n"
     ]
    }
   ],
   "source": [
    "# Divide o texto e pega palavras iniciais/finais\n",
    "chunks = dividir_em_chunks_com_palavras(documento)\n",
    "\n",
    "# Exemplo de saída para o primeiro chunk\n",
    "print(\"3 primeiras palavras:\", chunks[0]['inicio'])\n",
    "print(\"3 últimas palavras:\", chunks[0]['fim'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rocm_py312",
   "language": "python",
   "name": "venv_rocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
