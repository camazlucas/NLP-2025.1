{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6fb944",
   "metadata": {},
   "source": [
    "# Carregando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a255f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290d7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Rodando em: GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verifica e configura o dispositivo (GPU/CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "print(f\"üîß Rodando em: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55543abe",
   "metadata": {},
   "source": [
    "# Fun√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286ebfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_documento(caminho_do_pdf):\n",
    "\n",
    "    with pdfplumber.open(caminho_do_pdf) as pdf:\n",
    "        texto = \" \".join(\n",
    "            page.extract_text() for page in pdf.pages \n",
    "            if page.extract_text()  # Ignora p√°ginas sem texto\n",
    "        )\n",
    "        texto = \" \".join(texto.split())  # Normaliza espa√ßos\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7c80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_em_chunks_tokenizados(texto):\n",
    "\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4113e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenizados(texto):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    janelas = []\n",
    "    \n",
    "    for i in range(0, len(tokens), stride):\n",
    "        janela = tokens[i:i + window_size]\n",
    "        texto_janela = tokenizer.convert_tokens_to_string(janela)\n",
    "        janelas.append(texto_janela)\n",
    "        \n",
    "        # Para se chegarmos ao final do texto\n",
    "        if i + window_size >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86bc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperar_chunks(pergunta, modelo_vetorizador, divisao_do_texto, k=3):\n",
    "    # Vetoriza a pergunta\n",
    "    vetor_pergunta = modelo_vetorizador.encode([pergunta], convert_to_tensor=False)\n",
    "    \n",
    "    # Vetoriza todos os chunks\n",
    "    vetores_chunks = modelo_vetorizador.encode(divisao_do_texto, convert_to_tensor=False)\n",
    "    \n",
    "    # Calcula similaridade\n",
    "    sim = cosine_similarity(vetor_pergunta, vetores_chunks)\n",
    "    \n",
    "    # Pega √≠ndices dos top-k chunks mais similares\n",
    "    top_k_indices = np.argsort(sim[0])[::-1][:k]\n",
    "    \n",
    "    chunks_relevantes = [divisao_do_texto[i] for i in top_k_indices]\n",
    "    \n",
    "    return chunks_relevantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24104788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_resposta(pergunta, chunks_relevantes):\n",
    "    contexto = \" \".join(chunks_relevantes)\n",
    "    prompt = f\"Contexto: {contexto} \\n\\nPergunta: {pergunta} \\n\\nResposta:\"\n",
    "    resposta = modelo_gerador(prompt, max_length=200)\n",
    "    return resposta[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da93f3",
   "metadata": {},
   "source": [
    "# Vari√°veis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ef9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tamanho da Janela de Contexto\n",
    "window_size=400\n",
    "\n",
    "#Overlap da Janela de Contexto\n",
    "stride=100\n",
    "\n",
    "#Tamanho da Chunk\n",
    "chunk_size = 400\n",
    "    \n",
    "#Overlap da Chunk\n",
    "overlap = 100\n",
    "\n",
    "#Tamanho M√°ximo da Pergunta\n",
    "tamanho_pergunta = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575006b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962fa09c24224c9f8082306327fad42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "modelo_tokenizador = \"pierreguillou/bert-large-cased-squad-v1.1-portuguese\"\n",
    "\n",
    "modelo_vetorizador = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "modelo_gerador = pipeline(\"text2text-generation\", model=\"facebook/bart-large\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1b043",
   "metadata": {},
   "source": [
    "# Tratamento do Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964af42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo BERTimbau pr√©-treinado para Q&A em Portugues\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_tokenizador)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model= modelo_tokenizador,  # Modelo em Portugues\n",
    "    tokenizer=modelo_tokenizador,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc972ced",
   "metadata": {},
   "source": [
    "## Selecionar Divis√£o do Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddc6b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "documento = carregar_documento('Cabelo Cacheado Maior.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a208a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisao em Janela Deslizante\n",
    "\n",
    "divisao_do_texto = sliding_window_tokenizados(documento)\n",
    "\n",
    "#Divisao em Chunks\n",
    "\n",
    "#divisao_do_texto = dividir_em_chunks_tokenizados(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec14aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar todos os chunks\n",
    "vetores = modelo_vetorizador.encode(divisao_do_texto, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ea963",
   "metadata": {},
   "source": [
    "# Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88089d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1861 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Resposta: as a√ß√£o de oportunidade, ou o a√ß√µimento, de uma forma√ß√£o ou uma condi√ß√£o, como a√ß√∫caro ou como oportuno, com o acesso ao s√©culo.A√ß√£o como como acontece como funciona. A√ß√£o √© como ficar como sempre.Alexa como mais comercial. Alexas como provocada. Aquelea comerca√ß√£o. Asegura√ß√£o aqueleas comer√ßa. Aquest√£o comercia. Aquela√ß√£o foi comerciada.Aqueleva√ß√£o f√≠sica. Elesalimento. Eclesi√°ria. Ecosistema. Econ√≥mico. Efic√°cia. Ecol√≥gica. Especialidade. Especialidade como empresa. Eso como efeitosa. Efeito. Esteja. Eseja.Esteja ou. Eu.Eu.Ou. O\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"Quais s√£o os tipos de cachos?\"\n",
    "\n",
    "chunks_relevantes = recuperar_chunks(pergunta, modelo_vetorizador, divisao_do_texto)\n",
    "\n",
    "resposta = gerar_resposta(pergunta, chunks_relevantes)\n",
    "\n",
    "print(\"üìù Resposta:\", resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm_py312",
   "language": "python",
   "name": "venv_rocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
